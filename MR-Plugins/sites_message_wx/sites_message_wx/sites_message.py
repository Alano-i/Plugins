#!/usr/bin/env python3
from mbot.core.plugins import plugin
from mbot.core.plugins import PluginContext, PluginMeta
from mbot.openapi import mbot_api
from typing import Dict, Any
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import time
import os
import sys
import yaml
from datetime import datetime
import re
import random
import requests
import logging
import sqlite3
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
server = mbot_api

_LOGGER = logging.getLogger(__name__)
site_url = {
    'chdbits': 'https://chdbits.co',
    'HDHome': 'https://hdhome.org',
    'hdsky': 'https://hdsky.me',
    'hdchina': 'https://hdchina.org',
    'mteam': 'https://kp.m-team.cc',
    'lemonhd': 'https://lemonhd.org',
    'ourbits': 'https://ourbits.club',
    'ssd': 'https://springsunday.net',
    'keepfrds': 'https://pt.keepfrds.com',
    'pterclub': 'https://pterclub.com',
    'hdatmos': 'https://hdatmos.club',
    'beitai': 'https://beitai.pt',
    'soulvoice': 'https://pt.soulvoice.club',
    'audiences': 'https://audiences.me',
    'nicept': 'https://nicept.net',
    'pthome': 'https://pthome.net',
    'HDarea': 'https://hdarea.co',
    'HDTime': 'https://hdtime.org',
    'hd4fans': 'https://hd4fans.org',
    'hddolby': 'https://hddolby.com',
    'eastgame': 'https://pt.eastgame.org',
    'hdfans': 'https://hdfans.org',
    'discfan': 'https://discfan.net',
    'btschool': 'https://pt.btschool.club',
    'HDZone': 'https://hdzone.me',
    'gainbound': 'https://gainbound.net',
    'azusa': 'https://azusa.wiki',
    'ultrahd': 'https://ultrahd.net',
    'hhan': 'https://hhanclub.top',
    'hares': 'https://club.hares.top',
    'tjupt': 'https://tjupt.org',
    'leaves': 'https://leaves.red'
}

@plugin.after_setup
def after_setup(plugin_meta: PluginMeta, config: Dict[str, Any]):
    global words,user_id
    user_id = config.get('uid')[0]
    words = config.get('word_ignore')
    # global corpid,corpsecret,agentid,touser,msg_media_id,notice_media_id
    # corpid = config.get('corpid')
    # corpsecret = config.get('corpsecret')
    # agentid = config.get('agentid')
    # touser = config.get('touser')
    # msg_media_id = config.get('msg_media_id')
    # notice_media_id = config.get('notice_media_id')
    
@plugin.task('sites_message', 'PTç«™å†…ä¿¡æ¨é€', cron_expression='0 9,19 * * *')
def task():
    time.sleep(random.randint(1, 120))
    _LOGGER.info('å¼€å§‹è·å–ç«™å†…ä¿¡å’Œå…¬å‘Š')
    main()
    _LOGGER.info('æ‰€æœ‰ç«™ç‚¹ç«™å†…ä¿¡å’Œå…¬å‘Šè·å–å®Œæˆ')

def sites_message():
    site_list = server.site.list()
    corpid, agentid, corpsecret = get_qywx_info()
    touser = get_qywx_user(user_id)
    _LOGGER.info(f'è·å–åˆ°çš„ä¼ä¸šå¾®ä¿¡ä¿¡æ¯:ã€Œagentid: {agentid} corpid: {corpid} corpsecret: {corpsecret} touser: {touser}ã€')
    if not agentid or not corpid or not corpsecret or not touser:
        _LOGGER.error('ä¼ä¸šå¾®ä¿¡ä¿¡æ¯è·å–å¤±è´¥æˆ–å¡«å†™ä¸å®Œæ•´')
        _LOGGER.error('åœ¨è®¾ç½®-è®¾ç½®ä¼ä¸šå¾®ä¿¡é¡µè®¾ç½®ï¼šã€Œagentidã€ï¼Œã€Œcorpidã€ï¼Œã€Œcorpsecretã€')
        _LOGGER.error('åœ¨ç”¨æˆ·ç®¡ç†é¡µè®¾ç½®ã€Œå¾®ä¿¡è´¦å·ã€ï¼Œè·å–æ–¹æ³•å‚è€ƒ: https://alanoo.notion.site/thumb_media_id-64f170f7dcd14202ac5abd6d0e5031fb')
        _LOGGER.error('PTç«™å†…ä¿¡æ¨é€è¿›ç¨‹ç»ˆæ­¢ï¼Œã€Œè¯·å…ˆåœ¨ç³»ç»Ÿä¸­è®¾ç½®å¥½ä¸Šè¿°å‚æ•°é‡è¯•ã€')
        sys.exit()
    for site in site_list:
        site_id = site.site_id
        site_name = site.site_name
        if not site_id:
            continue
        if site_id not in site_url:
            continue
        _LOGGER.info(f'å¼€å§‹è·å–ã€Œ{site_name}ã€ç«™å†…ä¿¡å’Œå…¬å‘Š')
        try:
            caption_content_list,count,message_url,message_item_url,notice_list = get_nexusphp_message(site_url[site_id], site.cookie, site.proxies, site_name)
            if caption_content_list or notice_list:
                access_token = getToken(corpid, corpsecret)
                image_path = f'/data/plugins/sites_message_wx/pic/{site_id}.gif'
                try:
                    # æ£€æŸ¥ image_path æŒ‡å‘çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(image_path):
                        if caption_content_list:
                            image_path = f'/data/plugins/sites_message_wx/pic/msg_default.gif'
                        elif notice_list:
                            image_path = f'/data/plugins/sites_message_wx/pic/notice_default.gif'
                except Exception as e:
                    _LOGGER.error(f'æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ—¶å‘ç”Ÿå¼‚å¸¸ï¼ŒåŸå› ï¼š{e}')
                thumb_media_id = get_media_id(site_name, access_token, image_path)
            if caption_content_list:
                if count > 1:
                    wecom_title = f'ğŸ’Œ {site_name}: {count} æ¡ç«™å†…æ–°ä¿¡æ¯'
                    wecom_content_m = ''.join(caption_content_list)
                    wecom_content_m = wecom_content_m.replace('<line>', '')
                    # å»æ‰é¦–å°¾æ¢è¡Œç¬¦
                    wecom_content_m = wecom_content_m.strip('\n')
                    wecom_content = wecom_content_m.replace('\n', '<br/>')
                    wecom_digest = re.sub(r'<.*?>', '', wecom_content_m) 
                    content_source_url = message_url
                else:
                    wecom_title = caption_content_list[0].split('<line>\n')[0]
                    wecom_content = caption_content_list[0].split('<line>\n')[1]
                    wecom_content = wecom_content.strip('\n')
                    wecom_title = wecom_title.replace('\n', '')
                    wecom_title = re.sub(r'<.*?>', '', wecom_title)
                    wecom_title = f'ğŸ’Œ {site_name}: {wecom_title}'
                    wecom_title = wecom_title.replace('ğŸ’¬ ', '')
                    wecom_title = wecom_title.replace('ä½ çš„ç§å­/å¸–å­æ”¶åˆ°é­”åŠ›å€¼å¥–åŠ±', 'æ”¶åˆ°é­”åŠ›å€¼å¥–åŠ±')
                    wecom_title = wecom_title.replace('æ‚¨æ­£åœ¨ä¸‹è½½æˆ–åšç§çš„ç§å­è¢«åˆ é™¤', 'ç§å­è¢«åˆ é™¤')
                    content_source_url = message_item_url
                    wecom_digest = re.sub(r'<.*?>', '', wecom_content)
                wecom_content = wecom_content.replace('\n', '<br/>')
                # æ¨é€ç«™å†…ä¿¡
                result = push_msg(access_token, touser, agentid, wecom_title, thumb_media_id, content_source_url, wecom_digest, wecom_content)
                _LOGGER.info(f'ã€Œ{site_name}ã€ğŸ’Œ æœ‰æ–°ç«™å†…ä¿¡ï¼Œä¼ä¸šå¾®ä¿¡æ¨é€ç»“æœ: {result}')
            else:
                _LOGGER.info(f'ã€Œ{site_name}ã€æ— æœªè¯»ç«™å†…ä¿¡ï¼Œæˆ–é€šè¿‡å…³é”®è¯è¿‡æ»¤åæ²¡æœ‰éœ€è¦æ¨é€çš„æ–°æ¶ˆæ¯')
            if notice_list:
                wecom_title = f'ğŸ“¢ {site_name}: {notice_list[1]}'
                wecom_content_m = f'<b><big>{notice_list[0]}</b></big>\n<small>{notice_list[2]}</small>'
                wecom_content = wecom_content_m.replace('\n', '<br/>')
                wecom_digest = re.sub(r'<.*?>', '', wecom_content_m)
                content_source_url = f'{site_url}'
                # æ¨é€å…¬å‘Š
                result = push_msg(access_token, touser, agentid, wecom_title, thumb_media_id, content_source_url, wecom_digest, wecom_content)
                _LOGGER.info(f'ã€Œ{site_name}ã€ğŸ“¢ æœ‰æ–°å…¬å‘Šï¼Œä¼ä¸šå¾®ä¿¡æ¨é€ç»“æœ: {result}')
            else:
                _LOGGER.info(f'ã€Œ{site_name}ã€æ— æ–°å…¬å‘Š')
        except Exception as e:
            _LOGGER.error(f'å‘ç”Ÿé”™è¯¯ï¼ŒåŸå› ï¼š{e}')
            continue

def get_qywx_user(id):
    result = ''
    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect('/data/db/main.db')
    cursor = conn.cursor()
    cursor.execute('SELECT qywx_user FROM user WHERE id=?', (id,))
    result = cursor.fetchone()
    result = result[0]
    # å…³é—­æ•°æ®åº“è¿æ¥
    conn.close()
    return result

def get_qywx_info():
    try:
        yml_file = "/data/conf/base_config.yml"
        with open(yml_file, encoding='utf-8') as f:
            yml_data = yaml.load(f, Loader=yaml.FullLoader)
        for item in yml_data['notify_channel']:
            agentid = item.get('agentid')
            corpid = item.get('corpid')
            corpsecret = item.get('corpsecret')
            if agentid is not None and corpid is not None and corpsecret is not None:
                return corpid, agentid, corpsecret
    except Exception as e:
        print(f'è·å–ã€Œä¼ä¸šå¾®ä¿¡é…ç½®ä¿¡æ¯ã€é”™è¯¯ï¼Œå¯èƒ½MRä¸­å¡«å†™çš„ä¿¡æ¯æœ‰è¯¯æˆ–ä¸å…¨: {e}')
        pass
    return '','',''

def getToken(corpid, corpsecret):
    # è·å–access_token
    url = 'https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid={}&corpsecret={}'.format(corpid, corpsecret)
    MAX_RETRIES = 3
    for i in range(MAX_RETRIES):
        try:
            r = requests.get(url)
            access_token = r.json()['access_token']
            break
        except requests.RequestException as e:
            _LOGGER.error(f'å¤„ç†å¼‚å¸¸ï¼ŒåŸå› ï¼š{e}')
            time.sleep(2)
    # å¦‚æœé‡è¯•äº† MAX_RETRIES æ¬¡åä»ç„¶å¤±è´¥ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç 
    if r is None:
        _LOGGER.error('è¯·æ±‚ã€Œaccess_tokenã€å¤±è´¥')
    else:
        return access_token

def get_media_id(site_name, access_token, image_path):
    media_id_info_new = {}
    # è·å–å½“å‰æ—¶é—´
    current_time = time.time()
    if server.common.get_cache('media_id_info', site_name):
         # è·å–å­˜åœ¨ç¼“å­˜ä¸­çš„æ—¶é—´å’Œmedia_id
        stored_time = server.common.get_cache('media_id_info', site_name)['stored_time']
        stored_time_datetime = datetime.fromtimestamp(stored_time)
        stored_time_str = stored_time_datetime.strftime("%Y-%m-%d %H:%M:%S")
        media_id = server.common.get_cache('media_id_info', site_name)['media_id']
        stored_modify_time = server.common.get_cache('media_id_info', site_name)['stored_modify_time']
        # stored_modify_time = '2022-10-10 22:22:22'
        _LOGGER.info(f'ã€Œ{site_name}ã€ç¼“å­˜çš„å°é¢å›¾ç‰‡ä¿®æ”¹æ—¶é—´: {stored_modify_time}')
        _LOGGER.info(f'ã€Œ{site_name}ã€ä¸Šæ¬¡ä¼ å›¾åˆ°ç´ æåº“çš„æ—¶é—´: {stored_time_str}, 3å¤©æœ‰æ•ˆ, è¿‡æœŸè‡ªåŠ¨å†æ¬¡ä¸Šä¼ è·å–æ–°çš„ media_id')
        media_id_dict = {media_id}
        _LOGGER.info(f'ã€Œ{site_name}ã€å½“å‰æ­£åœ¨ä½¿ç”¨(ç¼“å­˜)çš„ ã€Œmedia_idã€: {media_id_dict}')
    else:
        _LOGGER.info(f'ã€Œ{site_name}ã€ç¼“å­˜çš„å°é¢å›¾ç‰‡ä¿®æ”¹æ—¶é—´: è¿˜æœªç¼“å­˜')
        _LOGGER.info(f'ã€Œ{site_name}ã€ä¸Šæ¬¡ä¼ å›¾åˆ°ç´ æåº“çš„æ—¶é—´: è¿˜æœªä¸Šä¼ è¿‡, 3å¤©æœ‰æ•ˆ, è¿‡æœŸè‡ªåŠ¨å†æ¬¡ä¸Šä¼ è·å–æ–°çš„ media_id')
        stored_time = current_time
        stored_modify_time = '2022-02-02 22:22:22'
        media_id = ''
    # è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
    current_modify_time = os.stat(image_path).st_mtime
    # æ ¼å¼åŒ–æ—¶é—´ä¸º"å¹´-æœˆ-æ—¥ æ—¶åˆ†ç§’"
    current_modify_time = datetime.fromtimestamp(current_modify_time)
    current_modify_time = current_modify_time.strftime("%Y-%m-%d %H:%M:%S")
    # å¦‚æœ å½“å‰æ—¶é—´ä¸å­˜å‚¨çš„æ—¶é—´å·®å¤§äº 3 å¤©ï¼Œå°±è°ƒç”¨ä¸Šä¼ å›¾ç‰‡çš„å‡½æ•°å¹¶é‡æ–°è·å– media_id
    if current_time - stored_time > 3 * 24 * 60 * 60 or not media_id or current_modify_time != stored_modify_time:
        media_id = upload_image_and_get_media_id(site_name, access_token, image_path)
        media_id_dict = {media_id}
        _LOGGER.info(f'ã€Œ{site_name}ã€ä¸Šä¼ å°é¢å›¾ç‰‡åè·å¾—çš„æœ€æ–°ã€Œmedia_idã€: {media_id_dict}')
        media_id_info_new = {'media_id':media_id, 'stored_time':current_time, 'stored_modify_time':current_modify_time}
        server.common.set_cache('media_id_info', site_name, media_id_info_new)
    else:
        pass
    stored_media_id_info = server.common.get_cache('media_id_info', site_name)
    _LOGGER.info(f'ã€Œ{site_name}ã€å·²ç¼“å­˜çš„ ã€Œmedia_id ä¿¡æ¯ã€: {stored_media_id_info}')
    return media_id
def upload_image_and_get_media_id(site_name, access_token, image_path):
    url = "https://qyapi.weixin.qq.com/cgi-bin/media/upload"
    # /cgi-bin/material/add_material æ°¸ä¹…ç´ ææ¥å£ï¼Œä½†éœ€è¦æˆæƒï¼Œä¸çŸ¥é“è¯¥æ€ä¹ˆæˆæƒ ï¼Œ/cgi-bin/media/upload ä¸´æ—¶ç´ ææ¥å£ï¼Œ3å¤©æœ‰æ•ˆ
    querystring = {"access_token": access_token, "type": "image"}
    files = {"media": ("image.gif", open(image_path, "rb"))}
    MAX_RETRIES = 3
    for i in range(MAX_RETRIES):
        try:
            response = requests.request("POST", url, params=querystring, files=files)
            break
        except requests.RequestException as e:
            _LOGGER.error(f'å¤„ç†å¼‚å¸¸ï¼ŒåŸå› ï¼š{e}')
            time.sleep(2)
    _LOGGER.info(f'ä¸Šä¼ å°é¢è¿”å›ç»“æœï¼š{response.text}')
    # è§£æå“åº”
    if response.status_code == 200:
        resp_data = response.json()
        media_id = resp_data.get('media_id')
        return media_id
        # _LOGGER.error(f'ä¸Šä¼ åè·å¾—çš„ã€Œmedia_idã€ï¼š{media_id}')
    else:
        _LOGGER.error(f'ä¸Šä¼ å›¾ç‰‡å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}')

def push_msg(access_token, touser, agentid, wecom_title, thumb_media_id, content_source_url, wecom_digest, wecom_content):
    # å‘é€æ¶ˆæ¯
    url = 'https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={}'.format(access_token)
    data = {
        "touser": touser,
        "msgtype": "mpnews",
        "agentid": agentid,
        "mpnews": {
            "articles": [
                {
                    "title" : wecom_title,
                    "thumb_media_id" : thumb_media_id,              # å¡ç‰‡å¤´éƒ¨å›¾ç‰‡é“¾æ¥ï¼Œæ­¤å›¾ç‰‡å­˜å‚¨åœ¨ä¼ä¸šå¾®ä¿¡ä¸­
                    "author" : "PTç«™å†…ä¿¡",                           # ç‚¹å‡»å¡ç‰‡è¿›å…¥ä¸‹çº§é¡µé¢åï¼Œæ—¶é—´æ—¥æœŸçš„æ—è¾¹çš„ä½œè€…
                    "content_source_url" : content_source_url,      # é˜…è¯»åŸæ–‡é“¾æ¥
                    "digest" : wecom_digest,                        # å›¾æ–‡æ¶ˆæ¯çš„æè¿°
                    "content" : wecom_content,                      # ç‚¹å‡»å¡ç‰‡è¿›å…¥ä¸‹çº§é¡µé¢åå±•ç¤ºçš„æ¶ˆæ¯å†…å®¹
                }
            ]
        },
        "safe": 0,
        "enable_id_trans": 0,
        "enable_duplicate_check": 0,
        "duplicate_check_interval": 1800
    }
    MAX_RETRIES = 3
    for i in range(MAX_RETRIES):
        try:
            r = requests.post(url, json=data)
            break
        except requests.RequestException as e:
            _LOGGER.error(f'å¤„ç†å¼‚å¸¸ï¼ŒåŸå› ï¼š{e}')
            time.sleep(2)
    if r is None:
        _LOGGER.error('è¯·æ±‚ã€Œæ¨é€æ¥å£ã€å¤±è´¥')
    else:
        return r.json()

def get_nexusphp_message(site_url, cookie, proxies, site_name):
    caption_content_list = []
    date_and_title = []
    notice_list = []
    sms_title = ''
    element_body = ''
    message_item_url = ''
    message_url = ''
    notice_url = ''
    xxx = ''
    count = 0

    notice_title_selector = 'td.text > div > a'
    notice_content_selector = 'td.text > div > div'

    unread_selector = 'td > img[alt="Unread"]'
    body_selector = 'td[colspan*="2"]'

    if proxies:
        if proxies.startswith('http'):
            proxies = {
                'http': proxies
            }
        elif proxies.startswith('socks5'):
            proxies = {
            'socks5': proxies
            }
    else:
        proxies = None

    # ç«™å†…ä¿¡
    message_url = '/messages.php?action=viewmailbox&box=1&unread=1'
    message_url = urljoin(site_url, message_url)
    headers = {
        'cookie': cookie,
    }
    session = requests.Session()
    retry = Retry(connect=3, backoff_factor=0.5)
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('https://', adapter)
    response = session.request("GET", message_url, headers=headers, proxies=proxies, timeout=30).text
    # response = requests.request("GET", message_url, headers=headers, proxies=proxies, timeout=30).text
    soup = BeautifulSoup(response, 'html.parser')
    unread_list = soup.select(unread_selector)
    for unread_item in unread_list:
        td = unread_item.parent.next_sibling.next_sibling
        sms_title = td.text
        sms_title = f'ğŸ’¬ {sms_title}'
        href = td.a['href']
        message_item_url = urljoin(site_url, href)
        message_res = session.request("GET", message_item_url, headers=headers, proxies=proxies, timeout=30).text
        # message_res = requests.request("GET", message_item_url, headers=headers, proxies=proxies, timeout=30).text
        message_soup = BeautifulSoup(message_res, 'html.parser')
        element_body = message_soup.select(body_selector)[0].text.strip()
        element_body = re.sub(r'[\n\r]+', '\n', element_body)
        element_body = re.sub(r'\[.*?\]', '', element_body)
        # è®¡æ•°
        count = count + 1
        caption_content = f'<b><big>{sms_title}</b></big><line>\n<small>{element_body}</small>\n\n'
        caption_content_list.append(caption_content)
    
    # è·å–æœ€æ–°å…¬å‘Š
    # notice_url = '/index.php'
    # notice_url = urljoin(site_url, notice_url)
    notice_url = site_url
    notice_response = session.request("GET", notice_url, headers=headers, proxies=proxies, timeout=30).text    
    soup = BeautifulSoup(notice_response, 'html.parser')
    # _LOGGER.error(f'soup: {soup}')
    date_and_title = soup.select(notice_title_selector)
    if date_and_title:
        date_and_title = date_and_title[0].text.strip()
        notice_date, notice_title = date_and_title.split(' - ')
        notice_date = notice_date.replace('.', '-')
        notice_date = f'{notice_date} å…¬å‘Š'
    else:
        notice_date = ''
        notice_title = ''
        # _LOGGER.error(f'ã€Œ{site_name}ã€è·å–å…¬å‘Šå¤±è´¥')

    notice_content = soup.select(notice_content_selector)
    if notice_content:
        notice_content = notice_content[0].text.strip()
        notice_content = notice_content.strip()
        notice_content = re.sub(r'[\n\r]+', '\n', notice_content)
        notice_content = re.sub(r'\[.*?\]', '', notice_content)
    else:
        notice_content = ''
    # notice_content = 'ç ”ç©¶å†³å®šæ˜å¤©ä¸ºåº†ç¥ç«™ç‚¹100å‘¨å¹´'

    if site_name == 'ä¸å¯è¯´'  and notice_content:
        notice_content = notice_content.replace('\nã€å‚ä¸è®¨è®ºã€‘', '')
        date_and_title, notice_content = notice_content.split(' \n')
        notice_content = notice_content.strip()
        date_and_title = date_and_title.strip()
        notice_date, notice_title = date_and_title.split(' - ')
        notice_date = notice_date.replace('.', '-')
        notice_date = f'{notice_date} æ–°å…¬å‘Š'
        
    # notice_list = [notice_date, notice_title, notice_content]

    # _LOGGER.error(f'date_and_title: {notice_content}')
    # _LOGGER.error(f'notice_content: {notice_content}')
    
    # notice_list = ['2022-12-28','ç«™ç‚¹å¼€é‚€é€šçŸ¥','ç ”ç©¶å†³å®šæ˜å¤©ä¸ºåº†ç¥ç«™ç‚¹100å‘¨å¹´ï¼Œå¼€æ”¾é‚€è¯·ï¼\n æœ›å‘¨çŸ¥ï¼Œç§¯æå‚åŠ ï¼']
    if notice_date and notice_title and notice_content:
    # if notice_list:
        new_notice = {'date':notice_date, 'title':notice_title, 'content':notice_content}
        # new_notice = {'date':notice_list[0], 'title':notice_list[1], 'content':notice_list[2]}
        old_notice = server.common.get_cache('site_notice', site_name)

        notice_list = [notice_date, notice_title, notice_content]
        
        if new_notice != old_notice:
            server.common.set_cache('site_notice', site_name, new_notice)
        else:
            notice_list = []
            _LOGGER.info(f'ã€Œ{site_name}ã€è·å–åˆ°çš„ã€Œæœ€æ–°å…¬å‘Šã€å’Œã€Œç¼“å­˜å…¬å‘Šã€ç›¸åŒï¼Œä¸æ¨é€')
            # _LOGGER.info(f'ã€Œ{site_name}ã€æ— æ–°å…¬å‘Š')
        
    else:
        _LOGGER.error(f'ã€Œ{site_name}ã€è·å–å…¬å‘Šå¤±è´¥')
        notice_list = ''
    
    # _LOGGER.error(f'notice_list: {notice_list}')

    xxx = server.common.get_cache('site_notice', site_name)
    _LOGGER.info(f'ã€Œ{site_name}ã€å…¬å‘Šçš„æœ€æ–°ç¼“å­˜ä¸º{xxx}')

    if caption_content_list:
        _LOGGER.info(f'ã€Œå…³é”®å­—è¿‡æ»¤å‰ï¼Œæœªè¯»ç«™å†…ä¿¡æ•°é‡ã€{count}')
        # å…³é”®å­—æ£€æŸ¥
        caption_content_list,count = word_ignore(site_name, caption_content_list,count)
        # _LOGGER.info(f'ã€Œç«™å†…ä¿¡ä¸»é¢˜ã€{sms_title}')
        # _LOGGER.info(f'ã€Œç«™å†…ä¿¡å†…å®¹ã€\n{element_body}')
        # _LOGGER.info(f'ã€Œå®Œæ•´ç«™å†…ä¿¡ã€\n{caption_list}')
        #_LOGGER.info(f'ã€Œæ’é™¤å…³é”®å­—åï¼Œç«™å†…ä¿¡è¯¦æƒ…é¡µã€\n{caption_content_list}')
        _LOGGER.info(f'ã€Œå…³é”®å­—è¿‡æ»¤åï¼Œæœªè¯»ç«™å†…ä¿¡æ•°é‡ã€{count}')
    # count = 3   
    # caption_content_list = ['ç«™ç‚¹å¼€é‚€é€šçŸ¥<line>\nè¿™æ˜¯å†…å®¹']
    # caption_content_list = ['<b><big>ğŸ’¬ ç­‰çº§å˜åŒ–</b></big><line>\n<small>ä½ è¢«é™çº§ä¸ºCrazy Userã€‚ç®¡ç†å‘˜ï¼šsidy</small>\n\n', "<b><big>ğŸ’¬ ç§å­è¢«åˆ é™¤</b></big><line>\n<small>ä½ æ­£åœ¨ä¸‹è½½æˆ–åšç§çš„ç§å­ ' The Mortal Ascention 2021 S01E71 1080p WEB-DL H264 AAC-OurTV'è¢«ç®¡ç†å‘˜[url=userdetails.php?id=159]admin[/url]åˆ é™¤ã€‚åŸå› ï¼šDupe!</small>\n\n", "<b><big>ğŸ’¬ æ¬¢è¿æ¥åˆ°OurBits!</b></big><line>\n<small>ç¥è´ºä½ ï¼Œ'ç«™ç‚¹ç”¨æˆ·å'ï¼Œ\nä½ å·²æˆä¸ºOurBitsçš„ä¸€å‘˜ï¼Œ\næˆ‘ä»¬çœŸè¯šåœ°æ¬¢è¿ä½ çš„åŠ å…¥ï¼\nè¯·åŠ¡å¿…å…ˆé˜…è¯»[url=rules.php][b]è§„åˆ™[/b][/url]ï¼Œæé—®å‰è¯·è‡ªè¡Œå‚è€ƒ[url=faq.php][b]å¸¸è§é—®é¢˜[/b][/url],æœ‰ç©ºä¹Ÿè¯·åˆ°[url=forums.php][b]è®ºå›[/b][/url]çœ‹çœ‹ã€‚ \nç¥ä½ æ„‰å¿«ã€‚\nOurBitsç®¡ç†ç»„</small>\n\n"]
    
    return caption_content_list,count,message_url,message_item_url,notice_list

def word_ignore(site_name, caption_content_list, count):
    word, hit = [], []
    if words:
        word = words.split(',')
        _LOGGER.info(f'ã€Œè®¾å®šè¿‡æ»¤å…³é”®è¯ã€{word}')
        for item in caption_content_list:
            for i in word:
                if i in item:
                    hit.append(item)
                    break
        # hit = set(hit)
        for hit_item in hit:
            caption_content_list.remove(hit_item)
            count = count - 1
            # _LOGGER.info(f'ã€Œ{site_name}ã€ç«™å†…ä¿¡ã€Œ{hit_item.strip()}ã€è§¦å‘å…³é”®è¯ï¼Œå·²å±è”½æ­¤æ¡ä¿¡æ¯ï¼')
            _LOGGER.error(f'ã€Œ{site_name}ã€æœªè¯»ç«™å†…ä¿¡è§¦å‘å…³é”®è¯è¿‡æ»¤ï¼Œå°†å±è”½æ­¤æ¡æ¶ˆæ¯ï¼Œç›¸å…³æ¶ˆæ¯ä¸ä¼šæ¨é€ï¼')
        if not hit:
            _LOGGER.info(f'ã€Œ{site_name}ã€æœªè¯»ç«™å†…ä¿¡æœªè§¦å‘å…³é”®è¯è¿‡æ»¤')
    else:
        _LOGGER.info(f'æœªè®¾å®šè¿‡æ»¤å…³é”®è¯')
    return caption_content_list,count
def main():
    sites_message()
